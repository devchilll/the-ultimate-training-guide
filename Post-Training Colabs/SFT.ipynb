{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VIB9meZdIY5p"
   },
   "source": [
    "# Supervised Fine-Tuning (SFT) with TRL and Qwen2-0.5B\n",
    "\n",
    "This Colab notebook demonstrates how to perform Supervised Fine-Tuning (SFT) using the trl library on a Qwen2-0.5B model. We will load a pre-trained base model, prepare a dataset (from gsm8k), and fine-tune the model to improve its performance on specific tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b2e717ce",
    "outputId": "1c8a66aa-c1f8-411f-c6e5-a80eee892b1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting trl\n",
      "  Downloading trl-0.27.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: accelerate>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from trl) (1.12.0)\n",
      "Requirement already satisfied: datasets>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from trl) (4.0.0)\n",
      "Requirement already satisfied: packaging>20.0 in /usr/local/lib/python3.12/dist-packages (from trl) (25.0)\n",
      "Requirement already satisfied: transformers>=4.56.2 in /usr/local/lib/python3.12/dist-packages (from trl) (4.57.6)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from accelerate>=1.4.0->trl) (2.0.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate>=1.4.0->trl) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from accelerate>=1.4.0->trl) (6.0.3)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate>=1.4.0->trl) (2.9.0+cu126)\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from accelerate>=1.4.0->trl) (0.36.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate>=1.4.0->trl) (0.7.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (3.20.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (2025.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.56.2->trl) (2025.11.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.56.2->trl) (0.22.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (3.13.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate>=1.4.0->trl) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate>=1.4.0->trl) (1.2.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (2026.1.4)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (3.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=3.0.0->trl) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=3.0.0->trl) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=3.0.0->trl) (2025.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (6.7.1)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.22.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=3.0.0->trl) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate>=1.4.0->trl) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate>=1.4.0->trl) (3.0.3)\n",
      "Downloading trl-0.27.1-py3-none-any.whl (532 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m532.9/532.9 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: trl\n",
      "Successfully installed trl-0.27.1\n"
     ]
    }
   ],
   "source": [
    "pip install trl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XxpTIL_OJKMc"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Hdf-nrqfH0VG"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import TrainingArguments, AutoTokenizer, AutoModelForCausalLM\n",
    "from trl import SFTTrainer, SFTConfig #, DataCollatorForCompletionOnlyLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t8jPTrd7Nr-z"
   },
   "source": [
    "## Check GPU - Colab Instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a3821d10",
    "outputId": "0c69d216-2d27-41c6-8a7b-235d4b3c9d22"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Current CUDA device: 0\n",
      "CUDA device name: Tesla T4\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if CUDA (GPU) is available\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Current CUDA device: {torch.cuda.current_device()}\")\n",
    "    print(f\"CUDA device name: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a9e4834b",
    "outputId": "aebd32e4-6051-4020-cc3c-d94bf509f50f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Jan 31 23:15:13 2026       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   39C    P8              9W /   70W |       2MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Check GPU memory usage (requires a running process, e.g., after loading the model)\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YE9dSF8VNp2V"
   },
   "source": [
    "## Global Param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ZaWfDlrgLlrD"
   },
   "outputs": [],
   "source": [
    "MAX_TOKEN_SIZE = 100\n",
    "MODEL_NAME = 'Qwen/Qwen2-0.5B'\n",
    "USE_ACCELERATOR = True # Changed from USE_GPU to USE_ACCELERATOR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OJnAGvgJJxJ5"
   },
   "source": [
    "## helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "uFRnnjwIJLCI"
   },
   "outputs": [],
   "source": [
    "def generate_responses(model, tokenizer, user_message, system_message=None,\n",
    "                       max_new_tokens=MAX_TOKEN_SIZE):\n",
    "    # Format chat using tokenizer's chat template\n",
    "    messages = []\n",
    "    if system_message:\n",
    "        messages.append({\"role\": \"system\", \"content\": system_message})\n",
    "\n",
    "    # We assume the data are all single-turn conversation\n",
    "    messages.append({\"role\": \"user\", \"content\": user_message})\n",
    "\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "        enable_thinking=False,\n",
    "    )\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device) # Ensure inputs are on the model's device\n",
    "    # Recommended to use vllm, sglang or TensorRT\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    input_len = inputs[\"input_ids\"].shape[1]\n",
    "    generated_ids = outputs[0][input_len:]\n",
    "    response = tokenizer.decode(generated_ids, skip_special_tokens=True).strip()\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "KqS14YszJ19F"
   },
   "outputs": [],
   "source": [
    "def test_model_with_questions(model, tokenizer, questions,\n",
    "                              system_message=None, title=\"Model Output\"):\n",
    "    print(f\"\\n=== {title} ===\")\n",
    "    for i, question in enumerate(questions, 1):\n",
    "        response = generate_responses(model, tokenizer, question,\n",
    "                                      system_message)\n",
    "        print(f\"\\nModel Input {i}:\\n{question}\\nModel Output {i}:\\n{response}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "5g2X9iQFJ74Q"
   },
   "outputs": [],
   "source": [
    "def load_model_and_tokenizer(model_name, use_accelerator=False):\n",
    "\n",
    "    # Load base model and tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "    if use_accelerator:\n",
    "        if torch.cuda.is_available():\n",
    "            device = \"cuda\"\n",
    "            print(f\"Using CUDA device: {device}\")\n",
    "        elif hasattr(xm, 'xla_device') and xm.xla_device().type == 'xla': # More robust XLA check\n",
    "            device = xm.xla_device()\n",
    "            print(f\"Using XLA device: {device}\")\n",
    "        else:\n",
    "            device = \"cpu\"\n",
    "            print(\"No accelerator found (CUDA or XLA), falling back to CPU.\")\n",
    "        model.to(device)\n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "        model.to(device)\n",
    "        print(\"Accelerator disabled, falling back to CPU.\")\n",
    "\n",
    "    if not tokenizer.chat_template:\n",
    "        tokenizer.chat_template = \"\"\"{% for message in messages %}\n",
    "                {% if message['role'] == 'system' %}System: {{ message['content'] }}\\n\n",
    "                {% elif message['role'] == 'user' %}User: {{ message['content'] }}\\n\n",
    "                {% elif message['role'] == 'assistant' %}Assistant: {{ message['content'] }} <|endoftext|>\n",
    "\n",
    "                {% endif %}\n",
    "                {% endfor %}\"\"\"\n",
    "\n",
    "    # Tokenizer config\n",
    "    if not tokenizer.pad_token:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "o9mytZnLJ-H3"
   },
   "outputs": [],
   "source": [
    "def display_dataset(dataset):\n",
    "    # Visualize the dataset\n",
    "    rows = []\n",
    "    for i in range(3):\n",
    "        example = dataset[i]\n",
    "        user_msg = next(m['content'] for m in example['messages']\n",
    "                        if m['role'] == 'user')\n",
    "        assistant_msg = next(m['content'] for m in example['messages']\n",
    "                             if m['role'] == 'assistant')\n",
    "        rows.append({\n",
    "            'User Prompt': user_msg,\n",
    "            'Assistant Response': assistant_msg\n",
    "        })\n",
    "\n",
    "    # Display as table\n",
    "    df = pd.DataFrame(rows)\n",
    "    pd.set_option('display.max_colwidth', None)  # Avoid truncating long strings\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DWDSpLM_KDim"
   },
   "source": [
    "## load base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "dTIAviihKK9m"
   },
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"Give me an 1-sentence introduction of LLM.\" ,\n",
    "    \"What's the difference between thread and process?\",\n",
    "    \"Calculate 2^4\",\n",
    "    \"I am a all mountain skier, give me a best carving ski recommendation!\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "4f1ed95faefd483ab740dae2309fe353",
      "f2b4ff2ae36b4d1980d29d78a22b7d1a",
      "5c032f653a78444cbeb31f95d375a4fd",
      "ec481f37d6be47b3be6c8f238555bc22",
      "f82996b0e14a457bbfde68e88100c797",
      "9246c357155d435a84849be84d565621",
      "f6b0be2445a84d39be7269e3ca2ca997",
      "50fd66a5cd644a028c785f3b4cd3cc00",
      "c8fe44ae50904edeb87e309939c61eb4",
      "f20223a823cc48e3b5cd2759979f89f2",
      "8e89283a3a7e4f3fb0bec299960efee5",
      "5a68a42109e14c56b11242c193bd0394",
      "bbbe32cb2524475b9ff615b0d1e3fa3d",
      "3a0ec18a89f0448ea38667027609ded1",
      "f5c45a41979343f5933a7e57c6eb1527",
      "e824347e63d7472ab691c75b92acde11",
      "6778419b01dc41dda7945692ba9c9d33",
      "d66f0f3355d94bcea29e02dc81242c0f",
      "649a6a047ef94b9d9be697aab496491b",
      "c93dcc671a4647f9bf8af6ba714696bb",
      "ce4bdf5dbd6c4660ab0f84837b8839a3",
      "43929910b73b4751b680204a34f357ee",
      "3570d1e931844ee497ef1b715fe45ca2",
      "0e5e0834674b4b53ac2134cb2a419cc7",
      "084ba674c22d4797a15226df20330849",
      "7e566469b1f94d3f9d72f649a43f79ef",
      "9a9bf8bfe4084457b614a929721a1e6c",
      "2f342da0c6bd4342b654fea9091b182b",
      "b4b042203dfc4a0a9ec71dbba76ec154",
      "cc9fce69d04840738e2030b24df64ddc",
      "94114a756dc041e9b5dabfd7b8d982de",
      "4f78779383504bd08b65fb3327193e1b",
      "43df733d5e254a5bbd83be00d97ac9a5",
      "7762448c774145df9435029e1b2ad9e4",
      "c055ca1a6a7143c8954cfd3160d9d996",
      "e09efdc51ba94ddc8c88cd0524980970",
      "b71ad08e825741b597f11458c8ea5d20",
      "99b4172ff19843bfa44933e498877a6a",
      "7e51631209aa45b8b8c30f26e9b044fa",
      "d6bb43b81ce94b878e5971435418c840",
      "009e525680024a4fb8b38badfbf9707e",
      "71bd8576b29b4887991f218c14367b82",
      "350ddef56f334d5aab2fdaf6283b07ca",
      "60c35a05749448a5bfce629cd88b1c46",
      "cf23a91e154a423688e86215aae4e335",
      "7676e9919a9b44f1af0e0cdcae13ba7f",
      "e3d87c025e6b4049908b09377e2144c9",
      "15931f42034a4066991581cb12fd2182",
      "3f151321b4444b8eb5f3d933d598db99",
      "2639ead035e34db6af5fbbc2eb3c0257",
      "a3f1b56fc5c647bfb7c8b26681ec7ba4",
      "9da05f6f9f4c4e4f9109e3445bfe2fd3",
      "90a55a2ff4324200877652515526353b",
      "51d925351e8e4d0eaa98808d452ffa77",
      "a7baa8110a474e859561b46ea5f49209",
      "be756132796b408985144cde251dbd0a",
      "c6d7ee5eddb0473390b7a8a0647aa6fe",
      "1b54ba9487084320b18db783952cd8a7",
      "d9b1469770614d5283c1021501b35dc4",
      "570aa863a7734a32a5ede18ac288a2a9",
      "4bb1df953ea748998417accd79040c83",
      "87721f2c3738426a93f3e2355fda3e94",
      "bf88cb2bc1ca4cbfab18b01b7c0ef963",
      "c7e6d08896fd4dc8995998fe764542b1",
      "497c2bc3de1b4bda9d2ae38227030cfb",
      "d1ea4d0859a642efb5ed028019fdbd46",
      "d634f65d64c64a2e8d97a57ee17943a9",
      "b7b5cd6d54614e40bbeabd970ae26f2b",
      "48060da35c7243dc93f8a9afa2e75c15",
      "6ccbae1240644a8ba4aa748aa86646e1",
      "89a9349b5b7d4905914aeeb749ae446d",
      "2e45f8eab5614b87a871dbbe43f87fc7",
      "00c21b5652a8474c8a2903790b55b215",
      "11294a9cbbbb46878b174ae9b50f0438",
      "8a3969fd4e444f7ea113c74e5b3fdde3",
      "347bfa0ad1244a10be81ade65f9e28a1",
      "a0bbd82752364beeaff2756de37dd6c7"
     ]
    },
    "id": "yUfcoogTKAzn",
    "outputId": "ca1c46ad-3bdf-4b63-c3a2-ddc9af59b299"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f1ed95faefd483ab740dae2309fe353",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a68a42109e14c56b11242c193bd0394",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3570d1e931844ee497ef1b715fe45ca2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7762448c774145df9435029e1b2ad9e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf23a91e154a423688e86215aae4e335",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/661 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be756132796b408985144cde251dbd0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/988M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d634f65d64c64a2e8d97a57ee17943a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/138 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA device: cuda\n",
      "\n",
      "=== Base Model (Before SFT) Output ===\n",
      "\n",
      "Model Input 1:\n",
      "Give me an 1-sentence introduction of LLM.\n",
      "Model Output 1:\n",
      "LLM stands for \"language model,\" which is a type of artificial intelligence that is trained to generate human-like text. It is used in natural language processing (NLP) tasks such as text generation, summarization, and question answering. LLMs are capable of generating text that is similar to human language, but they are not capable of understanding or generating human-like emotions or opinions. LLMs are used in various applications such as chatbots, virtual assistants, and natural language processing systems.\n",
      "\n",
      "\n",
      "Model Input 2:\n",
      "What's the difference between thread and process?\n",
      "Model Output 2:\n",
      "The main difference between a thread and a process is that a thread is a small, isolated unit of execution that is executed in a separate process, while a process is a larger, more complex unit of execution that is executed in a separate process.\n",
      "What is the solution to this math problem? What is the value of the expression 12 × 24 + 36 × 12?\n",
      "What is the value of the expression 12 × 24 +\n",
      "\n",
      "\n",
      "Model Input 3:\n",
      "Calculate 2^4\n",
      "Model Output 3:\n",
      "To calculate 2^4, we can use the exponentiation rule for powers of 2, which states that:\n",
      "\n",
      "a^m * a^n = a^(m+n)\n",
      "\n",
      "In this case, we have:\n",
      "\n",
      "2^4 = 2 * 2 * 2 * 2\n",
      "\n",
      "So, 2^4 = 2 * 2 * 2 * 2 = 16.\n",
      "\n",
      "\n",
      "Model Input 4:\n",
      "I am a all mountain skier, give me a best carving ski recommendation!\n",
      "Model Output 4:\n",
      "Based on your skiing experience, a best carving ski recommendation would be the Black Diamond 1000. \n",
      "\n",
      "The Black Diamond 1000 is a high-performance ski with a 1000mm length and a 1000mm width. It is designed for all-mountain skiing and is known for its excellent carving ability. It has a 1000mm length that allows for a wide range of skiing angles, making it ideal for carving and maneuvering\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = load_model_and_tokenizer(MODEL_NAME, USE_ACCELERATOR)\n",
    "\n",
    "test_model_with_questions(model, tokenizer, questions,\n",
    "                          title=\"Base Model (Before SFT) Output\")\n",
    "\n",
    "del model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sCejtBAGNvTd"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "19c06815"
   },
   "source": [
    "## Prepare the Dataset for SFT\n",
    "\n",
    "We need a dataset in a specific format for `SFTTrainer`. Each entry should be a dictionary containing a `messages` key, which holds a list of message dictionaries (role, content). We use `openai/gsm8k` dataset which contains grad school math problems. We take a small subset of that to conduct SFT on Qwen0.5b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 457,
     "referenced_widgets": [
      "02053ada58cd451dad0227d9dab9f6e6",
      "8373d86219b84f17bd8a697687fee5dd",
      "7ae47ba841c940f298bbbdf60b42f5e8",
      "0df90bb9200d41d9b95412eaeb58bbf0",
      "b7bbf5e2c0ae4022b1660c9b19e92be2",
      "1649e56e6f6e48a9af504b0446be9413",
      "3a64cfd636b6480b8c9c878ff6930aba",
      "08b7134e10424cff87acf8287efeb81f",
      "0c1f629ee826435cbe721ac52f515873",
      "ba561f7f8d234f37be2c5d877520fb9a",
      "132988014b124d6e954f5908b989189f",
      "fbec18bbd4bd483085e25f30b206c723",
      "f8ea6d85c2f74e4c9bc23fe14815ec00",
      "47799c9b8adb4e1eb2983e6fffce388b",
      "0647696d42574e1a9becf2c7c28ef3c8",
      "9c5b3e9a24444f15bf64ecb8ebaaa8d9",
      "87f46b7d7b07452e9d74751e35b74397",
      "e0203e95f4f142b99e68be9df5f10e9c",
      "8ff06fec89f448e89bc701a8e2ddadc5",
      "704f3ab7e5ed4d0db2f7f2c08eced58c",
      "1e1686d568c340b5a0792159f68495a5",
      "99b2617c3efa46a8961a504b3f02a317",
      "a229188a6b4047f8abce6c026709e0c7",
      "4f2966a348e84e2dbf476383ba61e652",
      "5bd5b0daa7e34b4784535e73b7b551db",
      "8d35586caf9f4b008522d9b88c0fd089",
      "4915ea5b1da84a5891aec554e87be9c2",
      "bd2f6b63622b4fd6b41c0231caa1430d",
      "59d7075ae79c47d0a83ce1de23a043b1",
      "52f0e7cfd52841849d94396f4fa898fd",
      "5164bf00020e4e28bdb81ef84a536c9c",
      "1f0b0b538cb34010a56fd306a525d165",
      "5ab582f153ca4f0fb8e6a6b63005b83c",
      "5b5efcaf1cc64454b48e6554a4b3e255",
      "58d6ab722ff242b89227eb9ea74162b2",
      "520cfa63404d41bab1b34ae1d980a822",
      "bf37a152a9164479a2cd9c2d8a6da739",
      "fd222adbbf2a4af8a57cd4ba6d79f601",
      "3dc80e0bb2804fdd836d1c4751d56013",
      "07586da7f4d04ffe9be3fad9308f8b13",
      "04de6b89fe6146c385e2448b1c4f2c52",
      "c83ecd973990491398d771bfeff2b511",
      "4ee08a36c8d547f1a544ad6c15c2bfd6",
      "910889fa55ef4225a802816719d9dc4a",
      "d6678815c7bc4dda8f158cd915671954",
      "9f258954f76740588e47945dae87f241",
      "0a7ac14ee4f746049a495771a3c57cf9",
      "5b5ea3198e334eec9aca92c2a3d38fc3",
      "8817a0e708d54ad9a47305bbd559e482",
      "53d06d66a975423c848a72458e403fb4",
      "adcaa2963e2a4208bef3c6219d5dd4da",
      "a74e609f5ef74281bc358c134bdecc06",
      "2f252f2d9c3b42919bbbed83e8be0efe",
      "0894f598f3654723b6edd1f9f246bfe5",
      "55298a1e81c44b1f869dcf76a0dedfe7",
      "b9e577ebc2e44620a835bc8e9cef402b",
      "83b5528e704d452da56b188209d95010",
      "b8b1e643e1b04ba793f1bd4fe724cb38",
      "6b28aaa585a54de8bc5a20891bdf6992",
      "fd0d94cda2794e3bb8bc2d1d795b201c",
      "b9262937f0624304834b8384367424c9",
      "d7e9e0fb06114010bae8e4d77c42c8ce",
      "9b88f7c4177c4b128c9b2dfec3fd80e8",
      "2c109a1804b54848a9722dd7031808f4",
      "c37108b7e256441ca8a02f1d753dafa7",
      "0dc2bbaa82ee46b794d2cc321e6c5712"
     ]
    },
    "id": "e_0bCmPXPQYU",
    "outputId": "a5f80000-96bb-4cb6-98a1-fbbfcb42b7a3"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02053ada58cd451dad0227d9dab9f6e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbec18bbd4bd483085e25f30b206c723",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "main/train-00000-of-00001.parquet:   0%|          | 0.00/2.31M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a229188a6b4047f8abce6c026709e0c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "main/test-00000-of-00001.parquet:   0%|          | 0.00/419k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b5efcaf1cc64454b48e6554a4b3e255",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/7473 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6678815c7bc4dda8f158cd915671954",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1319 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9e577ebc2e44620a835bc8e9cef402b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7473 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted GSM8K Dataset (first 3 examples):\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"display_dataset(dataset)\",\n  \"rows\": 3,\n  \"fields\": [\n    {\n      \"column\": \"User Prompt\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?\",\n          \"Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn?\",\n          \"Betty is saving money for a new wallet which costs $100. Betty has only half of the money she needs. Her parents decided to give her $15 for that purpose, and her grandparents twice as much as her parents. How much more money does Betty need to buy the wallet?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Assistant Response\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Natalia sold 48/2 = <<48/2=24>>24 clips in May.\\nNatalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\\n#### 72\",\n          \"Weng earns 12/60 = $<<12/60=0.2>>0.2 per minute.\\nWorking 50 minutes, she earned 0.2 x 50 = $<<0.2*50=10>>10.\\n#### 10\",\n          \"In the beginning, Betty has only 100 / 2 = $<<100/2=50>>50.\\nBetty's grandparents gave her 15 * 2 = $<<15*2=30>>30.\\nThis means, Betty needs 100 - 50 - 30 - 15 = $<<100-50-30-15=5>>5 more.\\n#### 5\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-aa11bc73-3fa6-4f36-b9d9-1f1dd8d9889b\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User Prompt</th>\n",
       "      <th>Assistant Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?</td>\n",
       "      <td>Natalia sold 48/2 = &lt;&lt;48/2=24&gt;&gt;24 clips in May.\\nNatalia sold 48+24 = &lt;&lt;48+24=72&gt;&gt;72 clips altogether in April and May.\\n#### 72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn?</td>\n",
       "      <td>Weng earns 12/60 = $&lt;&lt;12/60=0.2&gt;&gt;0.2 per minute.\\nWorking 50 minutes, she earned 0.2 x 50 = $&lt;&lt;0.2*50=10&gt;&gt;10.\\n#### 10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Betty is saving money for a new wallet which costs $100. Betty has only half of the money she needs. Her parents decided to give her $15 for that purpose, and her grandparents twice as much as her parents. How much more money does Betty need to buy the wallet?</td>\n",
       "      <td>In the beginning, Betty has only 100 / 2 = $&lt;&lt;100/2=50&gt;&gt;50.\\nBetty's grandparents gave her 15 * 2 = $&lt;&lt;15*2=30&gt;&gt;30.\\nThis means, Betty needs 100 - 50 - 30 - 15 = $&lt;&lt;100-50-30-15=5&gt;&gt;5 more.\\n#### 5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-aa11bc73-3fa6-4f36-b9d9-1f1dd8d9889b')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-aa11bc73-3fa6-4f36-b9d9-1f1dd8d9889b button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-aa11bc73-3fa6-4f36-b9d9-1f1dd8d9889b');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                            User Prompt  \\\n",
       "0                                                                                                           Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?   \n",
       "1                                                                                                                                                     Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn?   \n",
       "2  Betty is saving money for a new wallet which costs $100. Betty has only half of the money she needs. Her parents decided to give her $15 for that purpose, and her grandparents twice as much as her parents. How much more money does Betty need to buy the wallet?   \n",
       "\n",
       "                                                                                                                                                                                     Assistant Response  \n",
       "0                                                                      Natalia sold 48/2 = <<48/2=24>>24 clips in May.\\nNatalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\\n#### 72  \n",
       "1                                                                                Weng earns 12/60 = $<<12/60=0.2>>0.2 per minute.\\nWorking 50 minutes, she earned 0.2 x 50 = $<<0.2*50=10>>10.\\n#### 10  \n",
       "2  In the beginning, Betty has only 100 / 2 = $<<100/2=50>>50.\\nBetty's grandparents gave her 15 * 2 = $<<15*2=30>>30.\\nThis means, Betty needs 100 - 50 - 30 - 15 = $<<100-50-30-15=5>>5 more.\\n#### 5  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load grad school math dataset:\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the gsm8k dataset\n",
    "# Using 'main' subset which contains train and test splits\n",
    "dataset_gsm8k = load_dataset(\"gsm8k\", \"main\")\n",
    "\n",
    "# Function to format the gsm8k data into the SFTTrainer's expected 'messages' format\n",
    "def format_gsm8k_example(example):\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": example[\"question\"]},\n",
    "            {\"role\": \"assistant\", \"content\": example[\"answer\"]}\n",
    "        ]\n",
    "    }\n",
    "\n",
    "# Apply the formatting function to the training split\n",
    "dataset = dataset_gsm8k[\"train\"].map(format_gsm8k_example, remove_columns=dataset_gsm8k[\"train\"].column_names)\n",
    "\n",
    "# Display the first few examples to verify the format\n",
    "print(\"Formatted GSM8K Dataset (first 3 examples):\")\n",
    "display_dataset(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6YN5UdyiRVN9",
    "outputId": "b7454111-c56a-48e3-f38a-8bcdb5459492"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7473\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "59VUHiGNRMwB"
   },
   "outputs": [],
   "source": [
    "train_dataset = dataset.select(range(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 248
    },
    "id": "nYXqRvX7WhkC",
    "outputId": "dd19ed96-2847-4cfd-e1eb-5369e96ef522"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"display_dataset(train_dataset)\",\n  \"rows\": 3,\n  \"fields\": [\n    {\n      \"column\": \"User Prompt\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?\",\n          \"Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn?\",\n          \"Betty is saving money for a new wallet which costs $100. Betty has only half of the money she needs. Her parents decided to give her $15 for that purpose, and her grandparents twice as much as her parents. How much more money does Betty need to buy the wallet?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Assistant Response\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Natalia sold 48/2 = <<48/2=24>>24 clips in May.\\nNatalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\\n#### 72\",\n          \"Weng earns 12/60 = $<<12/60=0.2>>0.2 per minute.\\nWorking 50 minutes, she earned 0.2 x 50 = $<<0.2*50=10>>10.\\n#### 10\",\n          \"In the beginning, Betty has only 100 / 2 = $<<100/2=50>>50.\\nBetty's grandparents gave her 15 * 2 = $<<15*2=30>>30.\\nThis means, Betty needs 100 - 50 - 30 - 15 = $<<100-50-30-15=5>>5 more.\\n#### 5\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-50dde623-2a62-4ca9-a7f9-6231d3555496\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User Prompt</th>\n",
       "      <th>Assistant Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?</td>\n",
       "      <td>Natalia sold 48/2 = &lt;&lt;48/2=24&gt;&gt;24 clips in May.\\nNatalia sold 48+24 = &lt;&lt;48+24=72&gt;&gt;72 clips altogether in April and May.\\n#### 72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn?</td>\n",
       "      <td>Weng earns 12/60 = $&lt;&lt;12/60=0.2&gt;&gt;0.2 per minute.\\nWorking 50 minutes, she earned 0.2 x 50 = $&lt;&lt;0.2*50=10&gt;&gt;10.\\n#### 10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Betty is saving money for a new wallet which costs $100. Betty has only half of the money she needs. Her parents decided to give her $15 for that purpose, and her grandparents twice as much as her parents. How much more money does Betty need to buy the wallet?</td>\n",
       "      <td>In the beginning, Betty has only 100 / 2 = $&lt;&lt;100/2=50&gt;&gt;50.\\nBetty's grandparents gave her 15 * 2 = $&lt;&lt;15*2=30&gt;&gt;30.\\nThis means, Betty needs 100 - 50 - 30 - 15 = $&lt;&lt;100-50-30-15=5&gt;&gt;5 more.\\n#### 5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-50dde623-2a62-4ca9-a7f9-6231d3555496')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-50dde623-2a62-4ca9-a7f9-6231d3555496 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-50dde623-2a62-4ca9-a7f9-6231d3555496');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                            User Prompt  \\\n",
       "0                                                                                                           Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?   \n",
       "1                                                                                                                                                     Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn?   \n",
       "2  Betty is saving money for a new wallet which costs $100. Betty has only half of the money she needs. Her parents decided to give her $15 for that purpose, and her grandparents twice as much as her parents. How much more money does Betty need to buy the wallet?   \n",
       "\n",
       "                                                                                                                                                                                     Assistant Response  \n",
       "0                                                                      Natalia sold 48/2 = <<48/2=24>>24 clips in May.\\nNatalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\\n#### 72  \n",
       "1                                                                                Weng earns 12/60 = $<<12/60=0.2>>0.2 per minute.\\nWorking 50 minutes, she earned 0.2 x 50 = $<<0.2*50=10>>10.\\n#### 10  \n",
       "2  In the beginning, Betty has only 100 / 2 = $<<100/2=50>>50.\\nBetty's grandparents gave her 15 * 2 = $<<15*2=30>>30.\\nThis means, Betty needs 100 - 50 - 30 - 15 = $<<100-50-30-15=5>>5 more.\\n#### 5  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_dataset(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tHL93knVP4i_"
   },
   "source": [
    "# Load base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HMW583qFP6Ag",
    "outputId": "08998aaa-290f-4b9b-bc54-befb53ceb2c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA device: cuda\n"
     ]
    }
   ],
   "source": [
    "base_model, tokenizer = load_model_and_tokenizer(MODEL_NAME, USE_ACCELERATOR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d081128e"
   },
   "source": [
    "## Configure and Run SFT Trainer\n",
    "\n",
    "Now, let's set up the SFTTrainer. We'll define some `SFTConfig` parameters for our training run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3_XZkXI1P3oq"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "cb5135e4"
   },
   "outputs": [],
   "source": [
    "sft_config = SFTConfig(\n",
    "    output_dir=\"./Qwen2-0.5B-SFT\", # Directory to save the model and logs\n",
    "    num_train_epochs=2, # Number of training epochs\n",
    "    per_device_train_batch_size=2, # Batch size per GPU/TPU core\n",
    "    gradient_accumulation_steps=8, # Number of updates steps to accumulate before performing a backward/update pass\n",
    "    gradient_checkpointing=True, # Enable gradient checkpointing for memory efficiency\n",
    "    # optim=\"paged_adamw_32bit\", # Optimizer to use\n",
    "    learning_rate=8e-5, # Learning rate\n",
    "    fp16=True, # Use mixed precision training\n",
    "    logging_steps=10, # Log every N steps\n",
    "    save_steps=20, # Save checkpoint every N steps\n",
    "    push_to_hub=False, # Whether to push the model to the Hugging Face Hub\n",
    "    # max_seq_length=MAX_TOKEN_SIZE, # Maximum sequence length for training\n",
    "    dataset_text_field=\"messages\", # The field in the dataset that contains the conversation messages\n",
    "    packing=True, # Pack multiple short examples into one longer sequence to save compute\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 185
    },
    "id": "zzU-h00rQ5x5",
    "outputId": "2cc10340-2bcf-40e6-ec1d-5693075de505"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:trl.trainer.sft_trainer:Padding-free training is enabled, but the attention implementation is not set to a supported flash attention variant. Padding-free training flattens batches into a single sequence, and only the following implementations are known to reliably support this: flash_attention_2, flash_attention_3, kernels-community/flash-attn2, kernels-community/flash-attn3, kernels-community/vllm-flash-attn3. Using other implementations may lead to unexpected behavior. To ensure compatibility, set `attn_implementation` in the model configuration to one of these supported options or verify that your attention mechanism can handle flattened sequences.\n",
      "WARNING:trl.trainer.sft_trainer:You are using packing, but the attention implementation is not set to a supported flash attention variant. Packing gathers multiple samples into a single sequence, and only the following implementations are known to reliably support this: flash_attention_2, flash_attention_3, kernels-community/flash-attn2, kernels-community/flash-attn3, kernels-community/vllm-flash-attn3. Using other implementations may lead to cross-contamination between samples. To avoid this, either disable packing by setting `packing=False`, or set `attn_implementation` in the model configuration to one of these supported options.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4/4 03:56, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4, training_loss=0.9295732975006104, metrics={'train_runtime': 245.3821, 'train_samples_per_second': 0.171, 'train_steps_per_second': 0.016, 'total_flos': 88652793934848.0, 'train_loss': 0.9295732975006104})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=base_model,\n",
    "    processing_class=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    args=sft_config,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "18473039"
   },
   "source": [
    "## Test the fine-tuned model\n",
    "\n",
    "After training, let's see how the fine-tuned model performs on the questions.\n",
    "\n",
    "Comment: this is absolutely overfitting - because the dataset is a small grad math problems and so it overfit to how grad math problems are answered in a formatted way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d270b3ee",
    "outputId": "922e2a58-26ff-49e6-999f-972d2b33413c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "Caching is incompatible with gradient checkpointing in Qwen2DecoderLayer. Setting `past_key_values=None`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fine-tuned Model (After SFT) Output ===\n",
      "\n",
      "Model Input 1:\n",
      "Give me an 1-sentence introduction of LLM.\n",
      "Model Output 1:\n",
      "L'In the function (1.\n",
      "\n",
      "\n",
      "Model Input 2:\n",
      "What's the difference between thread and process?\n",
      "Model Output 2:\n",
      "The following\n",
      "A.\n",
      "\n",
      "\n",
      "Model Input 3:\n",
      "Calculate 2^4\n",
      "Model Output 3:\n",
      "To ensure that is a.\n",
      "\n",
      "\n",
      "Model Input 4:\n",
      "I am a all mountain skier, give me a best carving ski recommendation!\n",
      "Model Output 4:\n",
      "Based on the function (1.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_model_with_questions(base_model, tokenizer, questions,\n",
    "                          title=\"Fine-tuned Model (After SFT) Output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A_GZQAIOY9wh",
    "outputId": "92c07a96-c375-4152-8e39-d550ddb5ece4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fine-tuned Model (After SFT) Output - 2 epochs ===\n",
      "\n",
      "Model Input 1:\n",
      "Give me an 1-sentence introduction of LLM.\n",
      "Model Output 1:\n",
      "Lapsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystem\n",
      "\n",
      "\n",
      "Model Input 2:\n",
      "What's the difference between thread and process?\n",
      "Model Output 2:\n",
      "Threadsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystem\n",
      "\n",
      "\n",
      "Model Input 3:\n",
      "Calculate 2^4\n",
      "Model Output 3:\n",
      "##Prefixsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystem\n",
      "\n",
      "\n",
      "Model Input 4:\n",
      "I am a all mountain skier, give me a best carving ski recommendation!\n",
      "Model Output 4:\n",
      "I havesystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystemsystem\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_model_with_questions(base_model, tokenizer, questions,\n",
    "                          title=\"Fine-tuned Model (After SFT) Output - 2 epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wMRvtWfmW9dt"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
