# Hands-On Labs: Post-Training Practical Roadmap

> Build technical intuition through progressive experimentsâ€”from training a tiny model to deploying a specialized agent.

---

## ğŸ“ Stage 0: Pre-Training Fundamentals (Week 1)

*Focus: Understanding training dynamics from scratch.*

### Core Lab
Train a **character-level language model** from scratch using [nanoGPT](https://github.com/karpathy/nanoGPT) or minGPT.

### Dataset
- Shakespeare text (~1MB) or TinyStories for simpler patterns
- Tokenize at character level (no BPE complexity)

### Key Skills
- Implementing attention and feedforward layers in PyTorch
- Understanding loss curves: what does "good" training look like?
- Hyperparameter sensitivity: learning rate, batch size, context length

### Success Criteria
âœ… Model generates coherent (if nonsensical) text after 1000 steps  
âœ… You can explain every line of the training loop  
âœ… Loss decreases smoothly without NaN/explosion  

### Compute Requirements
ğŸ–¥ï¸ **Colab Free Tier** â€” CPU/T4 sufficient for character-level models

---

## ğŸ“ Stage 1: SFT Foundations (Weeks 2-3)

*Focus: Mastering the Supervised Fine-Tuning loop.*

### Core Lab
Fine-tune **Llama-3.2-1B** or **Qwen2.5-1.5B** using **Unsloth** for efficient training.

### Dataset
- `yahma/alpaca-cleaned` â€” standard instruction-following
- Or create your own: 500-1000 high-quality examples in a specific domain

### Key Skills
- Loading 4-bit quantized models with `bitsandbytes`
- LoRA configuration: rank, alpha, target modules
- Formatting chat templates correctly (system/user/assistant)
- Monitoring loss curves and identifying overfitting

### Success Criteria
âœ… Fine-tuned model follows instructions in your target format  
âœ… You can explain LoRA's memory savings mathematically  
âœ… Loss plateaus appropriately (not memorizing, not underfitting)  

### Compute Requirements
ğŸ–¥ï¸ **Colab Free Tier (T4)** â€” 1B models fit comfortably with 4-bit quantization  
ğŸ’° **Colab Pro (A100)** â€” for 7B+ models or faster iteration

---

## ğŸ“ Stage 2: Preference Alignment with DPO (Weeks 4-5)

*Focus: Learning from comparisons without explicit reward models.*

### Core Lab
Implement **Direct Preference Optimization (DPO)** to align a model with a specific persona or safety profile.

### Dataset
- Build preference triplets: `(prompt, chosen_response, rejected_response)`
- Sources: [Anthropic HH-RLHF](https://huggingface.co/datasets/Anthropic/hh-rlhf), or create 200+ pairs manually

### Key Skills
- Understanding the DPO loss derivation from RLHF
- Creating high-quality preference data
- Measuring "win-rate" vs. base model using LLM-as-Judge
- Implementing SafeDPO for jailbreak resistance

### Success Criteria
âœ… Aligned model shows measurable style/behavior shift  
âœ… Win-rate vs. base model >60% on held-out prompts  
âœ… You can derive the DPO loss from scratch  

### Compute Requirements
ğŸ–¥ï¸ **Colab Pro (A100)** â€” DPO requires reference model inference  
ğŸ’¡ Use gradient checkpointing to fit on T4 with smaller models

---

## ğŸ“ Stage 3: RL for Reasoning with GRPO (Weeks 6-8)

*Focus: Reproducing DeepSeek-R1 style reasoning.*

### Core Lab
Train a reasoning model on math/logic tasks using **Group Relative Policy Optimization (GRPO)** with Unsloth.

### Dataset
- **GSM8K**: Grade-school math with chain-of-thought
- **MATH**: Competition-level problems (harder)
- Format with `<think>...</think>` tags for reasoning traces

### Key Skills
- Designing custom reward functions (format + correctness verification)
- The "Cold Start" problem: bootstrapping RL with SFT on high-quality traces
- Understanding group-relative advantage estimation
- Debugging RL instabilities (reward hacking, mode collapse)

### Success Criteria
âœ… Model produces reasoning traces with `<think>` tags  
âœ… Accuracy improves on GSM8K test set  
âœ… You can explain GRPO's advantage over PPO for this setting  

### Compute Requirements
ğŸ’° **Colab Pro+ / A100** â€” RL requires multiple forward passes per step  
ğŸ–¥ï¸ **Local RTX 3090/4090** â€” 24GB VRAM ideal for 7B models

---

## ğŸ“ Stage 4: Tool Use & Function Calling (Weeks 9-10)

*Focus: Building a production-ready specialist agent.*

### Core Lab
Fine-tune a **Qwen2.5-7B** or **Llama-3.1-8B** for structured **function calling** and tool use.

### Dataset
- [Berkeley Function Calling Leaderboard (BFCL)](https://gorilla.cs.berkeley.edu/leaderboard.html) data
- Or create domain-specific tool schemas (weather, calendar, database)

### Key Skills
- Schema-aware fine-tuning for JSON function calls
- Achieving >90% accuracy on specific APIs
- Multi-LoRA serving: dynamically swap domain adapters on vLLM
- LLM-as-Judge evaluation for tool-use quality

### Success Criteria
âœ… Model correctly calls functions with proper arguments  
âœ… Outperforms base model on your domain's tool schemas  
âœ… Deploy with vLLM and hot-swap between LoRA adapters  

### Compute Requirements
ğŸ’° **Colab Pro+ / A100** â€” 7B+ models need significant VRAM  
ğŸ–¥ï¸ **Local GPU** â€” vLLM serving benefits from dedicated hardware

---

## ğŸ“ Stage 5: Evaluation & Red-Teaming (Weeks 11-12)

*Focus: Rigorous testing before claiming success.*

### Core Lab
Build a comprehensive **evaluation suite** for your fine-tuned models.

### Components
- **Automated benchmarks**: GSM8K, MMLU subset, HumanEval
- **LLM-as-Judge pipeline**: Rubric-based scoring with GPT-4/Claude
- **Red-teaming**: Test for jailbreaks, prompt injections, refusal calibration

### Key Skills
- Designing evaluation rubrics that correlate with human preference
- Statistical significance testing (is improvement real?)
- Identifying reward hacking and distributional shift

### Success Criteria
âœ… Eval suite runs end-to-end with clear metrics  
âœ… You can explain limitations of your metrics  
âœ… Red-teaming reveals no critical safety failures  

### Compute Requirements
ğŸ–¥ï¸ **Colab Free Tier** â€” evaluation is mostly inference  
ğŸ’° **API costs** â€” if using GPT-4/Claude as judges

---

## ğŸ“ Suggested Repository Structure

```
/labs
â”œâ”€â”€ 00_pretraining_fundamentals/
â”‚   â”œâ”€â”€ nanogpt_from_scratch.ipynb    # Train tiny LM
â”‚   â””â”€â”€ attention_implementation.py   # Hand-coded attention
â”œâ”€â”€ 01_sft_foundations/
â”‚   â”œâ”€â”€ unsloth_sft_tutorial.ipynb    # 4-bit LoRA fine-tuning
â”‚   â””â”€â”€ data_formatting.py            # Chat template utilities
â”œâ”€â”€ 02_preference_alignment/
â”‚   â”œâ”€â”€ dpo_training.ipynb            # DPO implementation
â”‚   â””â”€â”€ preference_data_creation.py   # Building triplet datasets
â”œâ”€â”€ 03_reasoning_rl/
â”‚   â”œâ”€â”€ grpo_reasoning.ipynb          # GRPO with custom rewards
â”‚   â””â”€â”€ reward_functions.py           # Format + correctness rewards
â”œâ”€â”€ 04_tool_use/
â”‚   â”œâ”€â”€ function_calling_sft.ipynb    # Schema-aware fine-tuning
â”‚   â””â”€â”€ vllm_multi_lora.py            # Multi-adapter serving
â””â”€â”€ 05_evaluation/
    â”œâ”€â”€ eval_suite.py                 # Automated benchmarking
    â””â”€â”€ llm_judge.py                  # Rubric-based evaluation

/infra
â”œâ”€â”€ vllm_config.yaml                  # Serving configuration
â”œâ”€â”€ quantization_guide.md             # AWQ/GPTQ/GGUF exports
â””â”€â”€ colab_setup.md                    # Environment setup tips
```

---

## ğŸ—“ï¸ Timeline Summary

| Stage | Focus | Duration | Compute |
|-------|-------|----------|---------|
| 0 | Pre-training fundamentals | 1 week | Colab Free |
| 1 | SFT with LoRA | 2 weeks | Colab Free/Pro |
| 2 | DPO preference alignment | 2 weeks | Colab Pro |
| 3 | GRPO reasoning RL | 3 weeks | Colab Pro+ / Local |
| 4 | Tool use & function calling | 2 weeks | Colab Pro+ / Local |
| 5 | Evaluation & red-teaming | 2 weeks | Colab Free + API |

**Total: ~12 weeks** to build comprehensive hands-on experience.