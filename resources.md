# Resources: Post-Training & Alignment

> Curated learning materials organized by topic.

---

## ğŸ“š Training Playbooks & Guides

- [**Smol Training Playbook**](https://huggingface.co/spaces/HuggingFaceTB/smol-training-playbook) â€” HuggingFace's comprehensive guide to training small LLMs
- [**LLM Course**](https://github.com/mlabonne/llm-course) â€” Complete roadmap to LLM engineering by Maxime Labonne
- [**RLHF Book**](https://rlhfbook.com/) â€” Nathan Lambert's free book on RLHF

---

## ğŸ›ï¸ Foundational Papers

### Transformers & Pre-training
- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) â€” Original Transformer paper
- [GPT-3: Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165) â€” OpenAI's scaling paper
- [LLaMA](https://arxiv.org/abs/2302.13971) â€” Meta's efficient pre-training recipe

### RLHF & Alignment
- [InstructGPT (RLHF)](https://arxiv.org/abs/2203.02155) â€” OpenAI's original RLHF paper
- [Constitutional AI](https://arxiv.org/abs/2212.08073) â€” Anthropic's RLAIF approach
- [DPO: Direct Preference Optimization](https://arxiv.org/abs/2305.18290) â€” RL-free preference learning

### Reasoning & RL
- [DeepSeek-R1](https://arxiv.org/abs/2501.12948) â€” GRPO for reasoning models
- [Let's Verify Step by Step](https://arxiv.org/abs/2305.20050) â€” Process reward models
- [STaR: Self-Taught Reasoner](https://arxiv.org/abs/2203.14465) â€” Bootstrapping reasoning

### Synthetic Data
- [Self-Instruct](https://arxiv.org/abs/2212.10560) â€” Generating instruction data
- [Evol-Instruct (WizardLM)](https://arxiv.org/abs/2304.12244) â€” Evolving instruction complexity
- [Orca 2](https://arxiv.org/abs/2311.11045) â€” Teaching small models to reason

---

## ğŸ”§ Tools & Frameworks

### Fine-Tuning
- [**Unsloth**](https://github.com/unslothai/unsloth) â€” 2x faster LoRA with 70% less VRAM
- [**Axolotl**](https://github.com/OpenAccess-AI-Collective/axolotl) â€” YAML-driven training pipelines
- [**LLaMA-Factory**](https://github.com/hiyouga/LLaMA-Factory) â€” Web UI for fine-tuning
- [**TRL**](https://github.com/huggingface/trl) â€” HuggingFace's RLHF library

### Inference & Serving
- [**vLLM**](https://github.com/vllm-project/vllm) â€” High-throughput serving with PagedAttention
- [**llama.cpp**](https://github.com/ggerganov/llama.cpp) â€” CPU/edge inference with GGUF
- [**Ollama**](https://ollama.ai/) â€” Local model deployment made easy

### Evaluation
- [**lm-eval-harness**](https://github.com/EleutherAI/lm-evaluation-harness) â€” Standard benchmark suite
- [**BFCL**](https://gorilla.cs.berkeley.edu/leaderboard.html) â€” Berkeley Function Calling Leaderboard
- [**DeepEval**](https://github.com/confident-ai/deepeval) â€” LLM evaluation framework

---

## ğŸ“ Courses & Lectures

- [**Karpathy: Let's build GPT**](https://www.youtube.com/watch?v=kCc8FmEb1nY) â€” Build GPT from scratch (2hr)
- [**Karpathy: nanoGPT**](https://github.com/karpathy/nanoGPT) â€” Minimal GPT training code
- [**Stanford CS224N**](https://web.stanford.edu/class/cs224n/) â€” NLP with Deep Learning
- [**Full Stack LLM Bootcamp**](https://fullstackdeeplearning.com/llm-bootcamp/) â€” Production LLM engineering

---

## ğŸ“ Technical Blogs

### OpenAI
- [ChatGPT](https://openai.com/index/chatgpt/) â€” Original ChatGPT announcement
- [GPT-4 Technical Report](https://openai.com/index/gpt-4-research/)
- [Scaling Laws](https://openai.com/index/scaling-laws-for-neural-language-models/)

### Anthropic
- [Claude's Character](https://www.anthropic.com/research/claude-character)
- [Core Views on AI Safety](https://www.anthropic.com/research/core-views-on-ai-safety)

### Others
- [Chip Huyen's Blog](https://huyenchip.com/blog/) â€” ML systems & LLM engineering
- [Sebastian Raschka](https://magazine.sebastianraschka.com/) â€” LLM fundamentals
- [Lil'Log (Lilian Weng)](https://lilianweng.github.io/) â€” Deep dives on RL, transformers

---

## ğŸ’¬ Communities

- [**r/LocalLLaMA**](https://reddit.com/r/LocalLLaMA) â€” Local model running & fine-tuning
- [**Latent Space Podcast**](https://www.latent.space/) â€” AI engineering podcast
- [**HuggingFace Discord**](https://discord.gg/huggingface) â€” Model training discussions

---

## ğŸ“Š Model Hubs & Datasets

- [**HuggingFace Hub**](https://huggingface.co/models) â€” Model repository
- [**Open LLM Leaderboard**](https://huggingface.co/spaces/HuggingFaceh4/open_llm_leaderboard) â€” Model benchmarks
- [**Anthropic HH-RLHF**](https://huggingface.co/datasets/Anthropic/hh-rlhf) â€” Human preference data
- [**OpenAssistant**](https://huggingface.co/datasets/OpenAssistant/oasst2) â€” Conversation data
